# Search-Scrape

**100% Free** web search and scraping MCP tools for AI assistants. No API keys, no costs, no limits.

## âœ¨ Features

- ðŸ” **Advanced Search**: Full SearXNG parameter support (engines, categories, language, safesearch, time_range, pagination)
- ðŸ•·ï¸ **Intelligent Scraping**: Smart content extraction with automatic noise filtering (ads, nav, footers removed)
- ðŸ”— **Smart Link Filtering**: Extracts links from main content area only, avoiding navigation clutter
- ðŸ“ **Source Citations**: Automatic reference-style `[1]`, `[2]` citations with clickable URLs in Sources section
- ðŸ§  **Agent-Friendly Extras**: SearXNG instant answers, related suggestions, spelling corrections, and unresponsive-engine warnings help agents self-start follow-ups
- ðŸŽ¯ **Configurable Limits**: Control `max_results`, `max_chars`, and `max_links` via request parameters or env vars to stay within token budgets
- ðŸ”§ **Native MCP Tools**: Direct integration with VS Code, Cursor, and other AI assistants
- ðŸ’° **100% Free**: No API keys or subscriptions required - runs completely locally
- ðŸ›¡ï¸ **Privacy First**: All processing happens on your machine
- âš¡ **Performance**: Built-in caching (10min search, 30min scrape), retry logic, and concurrency control
- ðŸŽ¨ **Content-Aware**: Special handling for documentation sites (mdBook, GitBook, etc.)

### ðŸ“¸ Screenshot

Here are screenshots showing the MCP tools working in Vscode, Cursor, Trae:

#### Search Web Tool
![Search Web Tool Screenshot](screenshot/search_web.png)

#### Scrape URL Tool  
![Scrape URL Tool Screenshot](screenshot/scrape_url.png)

## ðŸš€ Quick Start

```bash
# 1. Start SearXNG search engine
docker-compose up searxng -d

# 2. Build MCP server
cd mcp-server && cargo build --release

# 3. Add to your AI assistant's MCP config:
{
  "mcpServers": {
    "search-scrape": {
      "command": "/path/to/mcp-server/target/release/search-scrape-mcp",
      "env": { 
        "SEARXNG_URL": "http://localhost:8888",
        "SEARXNG_ENGINES": "google,bing,duckduckgo",
        "MAX_LINKS": "100"
      }
    }
  }
}
```

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `SEARXNG_URL` | `http://localhost:8888` | SearXNG instance URL |
| `SEARXNG_ENGINES` | `duckduckgo,google,bing` | Default search engines (comma-separated) |
| `MAX_LINKS` | `100` | Max links to return in Sources section |
| `MAX_CONTENT_CHARS` | `10000` | Default `max_chars` limit for scraped content (100-50000) |
| `RUST_LOG` | - | Log level: `error`, `warn`, `info`, `debug`, `trace` |

## ï¿½ MCP Tools

### `search_web` - Advanced Web Search
**Enhanced with full SearXNG parameter support:**
- **engines**: `google`, `bing`, `duckduckgo`, etc.
- **categories**: `general`, `news`, `it`, `science`, etc.
- **language**: `en`, `es`, `fr`, `de`, etc.
- **safesearch**: `0` (off), `1` (moderate), `2` (strict)
- **time_range**: `day`, `week`, `month`, `year`
- **pageno**: Page number for pagination

```json
{
  "query": "rust programming",
  "engines": "google,bing",
  "categories": "it,general",
  "language": "en",
  "safesearch": 1,
  "time_range": "month",
  "max_results": 20
}
```

**Agent-friendly extras:**
- `max_results`: Limit how many ranked results you return to keep the response concise (1-100, default: 10)
- The tool surfaces SearXNG `answers`, spelling `corrections`, `suggestions`, and a count of `unresponsive_engines` so agents know when to retry or refine the query

### `scrape_url` - Optimized Content Extraction
**Intelligent scraping with advanced cleanup:**
- âœ… **Smart Link Filtering**: Extracts links from main content (article/main tags) only
- âœ… **Source Citations**: Returns `[1]`, `[2]` markers with full URL mapping in Sources section
- âœ… **Noise Removal**: Automatically removes ads, navigation, footers, and boilerplate
- âœ… **Clean Text**: Extracts article text with proper formatting preserved
- âœ… **Rich Metadata**: OpenGraph, author, publish date, reading time, canonical URL
- âœ… **Structured Data**: Headings (H1-H6), images with alt text, language detection
- âœ… **Documentation Sites**: Special handling for mdBook, GitBook, and similar formats
- âœ… **Fallback Methods**: Multiple extraction strategies for difficult sites
- âœ… **Token-aware trimming**: `max_chars` keeps previews within a manageable length and shows a flag when the content is truncated
- âœ… **Configurable**: Control link/image limits and filtering behavior

**Parameters:**
```json
{
  "url": "https://doc.rust-lang.org/book/ch01-00-getting-started.html",
  "content_links_only": true,  // Optional: smart filter (default: true)
  "max_links": 100,            // Optional: limit sources (default: 100, max: 500)
  "max_chars": 10000           // Optional: cap preview length (default: 10000, max: 50000)
}
```

`max_chars` keeps scraped previews within token budgets; override the default for the entire server with the `MAX_CONTENT_CHARS` env var (100-50000).

**Example Output:**
```markdown
**Getting Started - The Rust Programming Language**

URL: https://doc.rust-lang.org/book/ch01-00-getting-started.html
Word Count: 842
Language: en

**Content:**
This chapter covers how to install Rust, write a Hello World program...
Learn more about [Cargo][1] and the [installation process][2].

**Sources:**
[1]: https://doc.rust-lang.org/cargo/ (Cargo documentation)
[2]: https://doc.rust-lang.org/book/ch01-01-installation.html (Installation)
[3]: https://doc.rust-lang.org/book/ch01-02-hello-world.html (Hello World)
...
```

## ðŸ› ï¸ Development

### HTTP API Testing
```bash
# Test search with parameters
curl -X POST "http://localhost:5000/search" \
  -H "Content-Type: application/json" \
  -d '{"query": "AI", "engines": "google", "language": "en"}'

# Test optimized scraping with smart filtering
curl -X POST "http://localhost:5000/scrape" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com", "content_links_only": true, "max_links": 50}'
```

### Running the Server
```bash
# HTTP server (port 5000)
cd mcp-server
SEARXNG_URL=http://localhost:8888 cargo run --release --bin mcp-server

# MCP stdio server (for AI assistants)
SEARXNG_URL=http://localhost:8888 ./target/release/search-scrape-mcp

# With debug logging
RUST_LOG=debug SEARXNG_URL=http://localhost:8888 cargo run --release
```

### Performance Tuning

**Cache Settings** (in `src/main.rs` and `src/stdio_service.rs`):
```rust
search_cache: 10_000 entries, 10 min TTL
scrape_cache: 10_000 entries, 30 min TTL
outbound_limit: 32 concurrent requests
```

**Optimization Tips:**
- Use `content_links_only: true` to reduce noise (enabled by default)
- Set `max_links` lower (e.g., 20-50) for faster responses
- Use `SEARXNG_ENGINES` env var to limit search engines
- Enable `RUST_LOG=info` for production monitoring

## ðŸ“ Project Structure

```
â”œâ”€â”€ mcp-server/           # Native Rust MCP server
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.rs       # HTTP server entry point
â”‚   â”‚   â”œâ”€â”€ stdio_service.rs  # MCP stdio server (for AI assistants)
â”‚   â”‚   â”œâ”€â”€ search.rs     # SearXNG integration with full parameter support  
â”‚   â”‚   â”œâ”€â”€ scrape.rs     # Scraping orchestration with caching & retry
â”‚   â”‚   â”œâ”€â”€ rust_scraper.rs   # Advanced extraction, noise filtering, smart links
â”‚   â”‚   â”œâ”€â”€ mcp.rs        # MCP HTTP endpoints
â”‚   â”‚   â”œâ”€â”€ types.rs      # Data structures & API types
â”‚   â”‚   â””â”€â”€ lib.rs        # Shared application state
â”‚   â””â”€â”€ target/release/   # Compiled binaries (mcp-server, search-scrape-mcp)
â”œâ”€â”€ searxng/             # SearXNG configuration
â”‚   â”œâ”€â”€ settings.yml     # Search engine settings
â”‚   â””â”€â”€ uwsgi.ini        # UWSGI server config
â””â”€â”€ docker-compose.yml   # Container orchestration
```

## ðŸ’¡ Best Practices

### ðŸ¤– For AI Agents (Auto-Follow These Guidelines)

**The tool descriptions already contain this guidance**, but here's a quick reference:

#### search_web Smart Usage
- **Always set `max_results`** based on your task:
  - Quick fact-check? â†’ `max_results: 5-10`
  - Balanced research? â†’ `max_results: 15-25`
  - Comprehensive survey? â†’ `max_results: 30-50`
- **Use `time_range`** for time-sensitive queries:
  - Breaking news â†’ `time_range: "day"`
  - Current events â†’ `time_range: "week"`
  - Recent trends â†’ `time_range: "month"`
- **Use `categories`** to filter results:
  - Technical/programming â†’ `categories: "it"`
  - News articles â†’ `categories: "news"`
  - Research papers â†’ `categories: "science"`
- **Check the response extras**:
  - Read `answers` field first (instant facts from SearXNG)
  - If you see `corrections`, retry with the suggested spelling
  - If `unresponsive_engines > 3`, consider retrying the query

#### scrape_url Smart Usage
- **Always adjust `max_chars`** based on your need:
  - Quick summary? â†’ `max_chars: 3000-5000`
  - Standard article? â†’ `max_chars: 10000` (default)
  - Long-form content? â†’ `max_chars: 20000-30000`
  - Full documentation? â†’ `max_chars: 40000+`
- **Keep `content_links_only: true`** (default) unless you specifically need nav/footer links
- **Check `word_count` in response**:
  - If < 50 words, the page may be JS-heavy or paywalled
  - Consider trying a different URL or informing the user
- **Use citation markers**: Content has `[1]`, `[2]` markers - reference the Sources section for specific URLs
- **Lower `max_links`** for faster responses when you don't need all sources

### For AI Assistants
- **Use smart filtering**: Keep `content_links_only: true` (default) to avoid nav/footer links
- **Limit result counts**: Dial back `max_results` to 5-20 when agents only need the top snippets
- **Cap preview length**: Use `max_chars` (or `MAX_CONTENT_CHARS`) to prevent huge scrape responses from draining tokens
- **Limit sources**: Set `max_links: 20-50` for cleaner responses when you don't need all links
- **Follow citations**: Use the `[1]`, `[2]` markers in content to find specific sources
- **Search first, scrape second**: Use `search_web` to find URLs, then `scrape_url` for deep content

### For Developers
- **Cache effectively**: Search results cached 10min, scrapes cached 30min
- **Handle errors gracefully**: Retry logic built-in (exponential backoff)
- **Monitor performance**: Use `RUST_LOG=info` to track cache hits and timing
- **Customize engines**: Set `SEARXNG_ENGINES` for domain-specific search
- **Rate limiting**: Built-in semaphore (32 concurrent) prevents overwhelming targets

### For Content Extraction
- **Documentation sites work great**: mdBook, GitBook auto-detected
- **JavaScript-heavy sites**: May have limited content (no JS execution)
- **Prefer canonical URLs**: Tool extracts canonical link when available
- **Reading time**: Automatically calculated at ~200 words/minute

## ðŸ”§ Troubleshooting

**SearXNG not responding:**
```bash
docker-compose restart searxng
# Check logs: docker-compose logs searxng
```

**Empty scrape results:**
- Site may be JavaScript-heavy (we don't execute JS)
- Try the URL in a browser to verify content is in HTML
- Check logs with `RUST_LOG=debug` for detailed extraction info

**Too many/too few links:**
- Adjust `max_links` parameter (default: 100, max: 500)
- Use `content_links_only: false` to get all document links
- Use `content_links_only: true` for main content only (default)

**Slow responses:**
- Check cache hit rates with `RUST_LOG=info`
- Verify SearXNG is running: `curl http://localhost:8888`
- Reduce concurrent load (outbound_limit in source)

## ðŸ¤ Contributing

Contributions welcome! Areas for improvement:
- Additional search engines in SearXNG config
- JavaScript execution support (headless browser)
- PDF/document extraction
- More smart content patterns
- Performance optimizations

## ðŸ“„ License

MIT License - Free to use, modify, and distribute.
