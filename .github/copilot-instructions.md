## MCP Tool Usage Rules (CortexAST + Shadowcrawl)

### CortexAST Priority Rules

**The Golden Rule (Nonâ€‘Negotiable):**
- NEVER use standard IDE/shell tools (`grep`, `rg`, `cat`, `head`, `tree`, `ls`, `git diff`) for codebase exploration, symbol lookup, or refactor verification.
- ALWAYS use CortexAST Megatools. They are AST-accurate, token-efficient, and designed to keep agents on rails.
- If a tool returns an error telling you which parameter you forgot, treat it as an instruction and retry the tool call (do not guess).

**Megatool Quickâ€‘Reference**

| Task | Megatool | Action Enum | Required Params |
|---|---|---|---|
| Repo overview (files + public symbols) | `cortex_code_explorer` | `map_overview` | `target_dir` (use `.` for whole repo) |
| Token-budgeted context slice (XML) | `cortex_code_explorer` | `deep_slice` | `target` |
| Extract exact symbol source | `cortex_symbol_analyzer` | `read_source` | `path` + `symbol_name` *(or `path` + `symbol_names` for batch)* |
| Find all usages before signature change | `cortex_symbol_analyzer` | `find_usages` | `symbol_name` + `target_dir` |
| Find trait/interface implementors | `cortex_symbol_analyzer` | `find_implementations` | `symbol_name` + `target_dir` |
| Blast radius before rename/move/delete | `cortex_symbol_analyzer` | `blast_radius` | `symbol_name` + `target_dir` |
| Cross-boundary update checklist | `cortex_symbol_analyzer` | `propagation_checklist` | `symbol_name` *(or legacy `changed_path`)* |
| Save pre-change snapshot | `cortex_chronos` | `save_checkpoint` | `path` + `symbol_name` + `semantic_tag` |
| List snapshots | `cortex_chronos` | `list_checkpoints` | *(none)* |
| Compare snapshots (AST diff) | `cortex_chronos` | `compare_checkpoint` | `symbol_name` + `tag_a` + `tag_b` *(use `tag_b="__live__"` + `path` to diff against current state)* |
| Delete old snapshots (housekeeping) | `cortex_chronos` | `delete_checkpoint` | `symbol_name` and/or `semantic_tag` *(optional: `path`, `namespace`)* â€” Automatically searches legacy flat `checkpoints/` if no matches in namespace. |
| Compile/lint diagnostics | `run_diagnostics` | *(none)* | `repoPath` |

## The Ultimate CortexAST Refactoring SOP

Whenever you are asked to perform a non-trivial refactor or update a core feature, you MUST generate and print this Markdown checklist into the chat **before writing any code**, and check the boxes as you proceed:

### ðŸ§  Refactoring Orchestration Plan
- [ ] **Phase 1: Recon & Blast Radius**
  - [ ] Use `map_overview` to understand the domain.
  - [ ] Use `blast_radius` (or `find_usages`) on the target symbol.
- [ ] **Phase 2: Snapshot**
  - [ ] Use `save_checkpoint` on the target files/symbols.
- [ ] **Phase 3: Execution**
  - [ ] Read minimal context using `read_source` (`skeleton_only: true` if large).
  - [ ] Write the code edits.
- [ ] **Phase 4: Verification & Sync**
  - [ ] Use `run_diagnostics` to catch compiler errors.
  - [ ] Use `compare_checkpoint` to verify structural intent.
  - [ ] Use `propagation_checklist` to ensure TS/Python/Proto boundaries are updated.

**The Autonomous Refactoring Flow (Rails)**

Follow this sequence for any non-trivial refactor (especially renames, signature changes, or cross-module work):

1. **Explore** â†’ `cortex_code_explorer(action: map_overview)`
2. **Isolate** â†’ `cortex_symbol_analyzer(action: read_source)` (get the exact symbol source before editing)
3. **Measure Impact** â†’
  - Use `cortex_symbol_analyzer(action: find_usages)` BEFORE changing any signature
  - Use `cortex_symbol_analyzer(action: blast_radius)` BEFORE any rename/move/delete
4. **Checkpoint** â†’ `cortex_chronos(action: save_checkpoint, semantic_tag: pre-refactor)`
5. **Edit Code** â†’ make the minimal change
6. **Verify** â†’
  - `run_diagnostics` immediately after editing
  - `cortex_chronos(action: compare_checkpoint)` to verify semantics (never use `git diff`); prefer `tag_b="__live__"` for "before vs now"
7. **Crossâ€‘Sync** â†’ `cortex_symbol_analyzer(action: propagation_checklist)` when touching shared types/contracts

**Output safety (spill prevention):**
- Output is truncated server-side at `max_chars` (default **8000**). VS Code Copilot writes responses larger than ~8 KB to workspace storage â€” the 8000 default is calibrated to stay below that threshold. Set `max_chars` explicitly (e.g. `3000`) for large-scope queries; increase only if your client handles larger inline output.

**`repoPath` best practice:**
- Always pass `repoPath` explicitly on every tool call (e.g. `repoPath="/Users/me/project"`) when you know the path. Without it, the server uses the root established by the MCP `initialize` handshake.
- **CRITICAL Safeguard:** For safety, if CortexAST resolves to a "dead root" (OS root or Home directory), it returns a **CRITICAL error** instead of proceeding. You MUST catch this and provide the correct `repoPath`.
- Use the absolute workspace root path, not a subdirectory.
- **Server owners**: configure `--root /absolute/path/to/project` in your MCP server args (or set `CORTEXAST_ROOT` env var). This is the most reliable fix â€” it sets the default root at server startup so every tool call resolves correctly even without an explicit `repoPath`. Example VS Code `settings.json`: `"args": ["mcp", "--root", "/Users/me/my-project"]`.
- **Root resolution order** (first non-dead value wins):
  1. `repoPath` param in the tool call â€” per-call override
  2. Root from the MCP `initialize` request (`rootUri` / `rootPath` / `workspaceFolders[0].uri`) â€” **the authoritative protocol-level value**; overwrites the bootstrap
  3. `--root` CLI flag / `CORTEXAST_ROOT` env var â€” startup bootstrap
  4. IDE env vars: `VSCODE_WORKSPACE_FOLDER`, `VSCODE_CWD`, `IDEA_INITIAL_DIRECTORY`, `PWD`/`INIT_CWD` (if â‰  `$HOME`) â€” checked both at startup AND directly inside every tool call (belt-and-suspenders)
  5. Find-up heuristic on the tool's own `path` / `target_dir` / `target` argument â€” walks ancestors looking for `.git`, `Cargo.toml`, `package.json`
  6. `cwd` â€” **refused if it equals `$HOME` or OS root** (CRITICAL error)

**Propagation best practice (Hybrid Omniâ€‘Match):**
- `propagation_checklist` automatically matches common casing variants of `symbol_name` (PascalCase / camelCase / snake_case).
- When a symbol is renamed across boundaries (e.g. Rust `TrainingEngineCapabilities` â†’ TS `trainingCaps`), pass `aliases: ["trainingCaps"]` to catch cross-language usage without heavy import tracing.

# MCP Usage Guidelines â€” ShadowCrawl

These rules encode the optimal agent workflow for ShadowCrawl tools.
They are hardcoded here so every AI agent working in this repository follows
the same efficient, hallucination-resistant research process.

---

## Shadowcrawl Priority Rules

### 1. Memory Before Search (mandatory â€” NEVER skip)
- **ALWAYS** call `memory_search` BEFORE calling `web_search`, `web_search_json`, **or** `web_fetch`
- If a result is returned with similarity score â‰¥ 0.60, use the cached data directly
  and skip the live fetch entirely
- Only proceed to a fresh live search when memory returns no relevant hit
- This rule applies to EVERY research cycle, including retries and follow-up fetches

### 1a. Dynamic Parameters â€” Always Tune for the Task (mandatory)
- **`max_chars` controls the TOTAL serialized output payload**, not just the text field.
  Increase it for large pages (e.g. `max_chars: 50000`), decrease for tight token budgets.
- **Agent-tunable parameters** â€” set these to match your task:
  | Param | Default | When to change |
  |---|---|---|
  | `max_chars` | 10000 | Increase for deep content; decrease for summaries |
  | `snippet_chars` | 120/200 | Increase for detailed research snippets |
  | `max_headings` | 10 | Decrease for summary output |
  | `max_images` | 3 | Increase for image-rich pages |
  | `short_content_threshold` | 50 | Adjust for minimal/sparse pages |
  | `extraction_score_threshold` | 0.4 | Lower for low-quality HTML pages |
  | `placeholder_word_threshold` | 10 | Tune for JS-heavy pages |

### 2. Prefer `web_search_json` Over `web_search` + `web_fetch`
- `web_search_json` combines search + pre-scraped content summaries in a **single call**
- Use `web_search_json` as the **default first step** for any research task
- Only fall back to `web_search` (without content) when you specifically need raw URLs only

### 3. Use `web_fetch` with Noise Reduction for Documentation Pages
- For documentation, article, or tutorial pages always set:
  ```
  output_format: "clean_json"
  strict_relevance: true
  query: "<your specific question>"
  ```
- This strips 100 % of nav/footer/boilerplate and keeps only query-relevant paragraphs
- Token savings are typically 60â€“80 % compared to raw text output
- If you see `clean_json_truncated` in warnings, increase `max_chars` (the tool clips large pages to prevent output spilling).
- Note: semantic shaving intentionally bypasses when `word_count < 200` (short pages are returned whole).
- **Raw file auto-detection**: when the URL ends in `.md`, `.mdx`, `.rst`, `.txt`, `.csv`, `.toml`, `.yaml`, or `.yml`, `clean_json` mode **automatically skips the HTML extraction pipeline** and returns the raw content directly â€” no duplicate frontmatter, no noise. The response will contain a `raw_markdown_url` warning. To read a raw GitHub file, prefer `web_fetch(output_format: "text")` for prose or `web_fetch(output_format: "clean_json")` for structured paragraphs.

### 4. Rotate Proxy on First Block Signal (mandatory)
- If `web_fetch` or `web_search` returns **403 / 429 / rate-limit / IP-block**:
  1. Immediately call `proxy_control` with `action: "grab"`
  2. Retry the failed call with `use_proxy: true`
- Do NOT retry the same call without rotating first; do NOT escalate to `hitl_web_fetch`
  until proxy rotation has also failed

### 4a. Auto-Escalation on Low Confidence (mandatory â€” no repeat prompt)
- If `web_fetch` returns `confidence < 0.3` OR `extraction_score < 0.4` in the response:
  1. **First**: retry with `quality_mode: "aggressive"` (triggers full CDP browser rendering)
  2. **If still failing**: automatically escalate to `visual_scout` to screenshot the page
  3. **If auth-wall confirmed**: escalate to `human_auth_session` WITHOUT waiting for further
     user instructions â€” surface the reason and proceed autonomously
- For `extract_structured` / `fetch_then_extract`: if response includes `confidence == 0.0`
  AND `placeholder_page` warning, immediately retry via `non_robot_search` (HITL/CDP)
- **Never** leave the agent stuck on a low-confidence result without attempting escalation

### 5. Structured Extraction â€” `fetch_then_extract` / `extract_structured`

Use schema-driven extraction when you need a stable JSON shape for downstream agent logic.

- Prefer `fetch_then_extract` for **one-shot** workflows (fetch + extract in the same tool call).
- Use `extract_structured` when you need schema extraction on an already-known URL (agent framework routes through it).
- **`raw_markdown_url` auto-warn**: both `extract_structured` and `fetch_then_extract` automatically inject a warning into `warnings[]` when the URL is a raw `.md`/`.mdx`/`.rst`/`.txt` file â€” fields will likely return `null` and confidence will be low. Use `web_fetch(output_format: "clean_json")` instead for raw Markdown sources.

Strict mode (default):
- `strict: true` enforces schema shape (no drift):
  - missing array-like fields â†’ `[]`
  - missing scalar fields â†’ `null`
- In strict mode with an explicit schema, metadata keys like `_title` / `_url` are suppressed.

Confidence safety (mandatory):
- `confidence == 0.0` + `placeholder_page` warning fires **only when BOTH of the following are true**:
  1. `word_count < placeholder_word_threshold` (default 10) **or** content has â‰¤ 1 non-empty line
  2. â‰¥ `placeholder_empty_ratio` (default 0.9) of **non-array** schema fields are null/empty string
  - **Empty arrays are excluded from condition 2** â€” `[]` is a valid "no items found" result and is
    never counted as a placeholder signal. A schema with only array fields (`structs`, `modules`, etc.)
    will **never** trigger confidence=0.0 via this path.
  - When fired: the page is an unrendered JS-only shell (e.g. crates.io, npm). Do NOT trust any fields.
    Escalate to CDP/browser rendering or HITL auth tools.
  - When NOT fired: partial extraction is valid â€” e.g. `structs: [32 items]` + `modules: []` stays
    above 0.0 because `structs` is a non-empty array (no scalar fields to fail the check).
- Tuning: pass `placeholder_word_threshold` (integer) and `placeholder_empty_ratio` (0â€“1 float) to
  `extract_structured` / `fetch_then_extract` when the defaults cause false positives or false negatives.

Input constraint:
- Donâ€™t point structured extraction at raw `.md` / `.json` / `.txt` unless thatâ€™s intentionally your source.
  For docs pages, prefer `web_fetch(output_format="clean_json")` then extract.- If you receive a `raw_markdown_url` warning in the response, switch to `web_fetch` instead.
### 6. `web_crawl` â€” Use When Sub-Page Discovery Is Needed
- Use `web_crawl` when you know a doc site's index URL but do not know which sub-page
  holds the information you need
- Do NOT assume a single `web_fetch` of the index page is sufficient for large doc sites
- Typical workflow: `web_crawl` to discover links â†’ `web_fetch` on the specific sub-page
- Output is capped at `max_chars` (default 10 000) to prevent workspace storage spill.
  Pass a higher value (e.g. `max_chars: 30000`) when crawling many pages.

### 7. `non_robot_search` â€” Last Resort Only
- Use ONLY when both direct fetch AND proxy rotation have failed
- Intended for: heavy Cloudflare challenges, CAPTCHA, login walls
- Do NOT use as a first attempt for any site â€” always try automated methods first
- After a successful HITL session, cookies are saved to `~/.shadowcrawl/sessions/{domain}.json` â€” future `web_fetch` calls to that domain are automatically authenticated

---

## Decision Flow Summary

```
Question / research task
        â”‚
        â–¼
research_history â”€â”€â–º hit (â‰¥ 0.60)? â”€â”€â–º use cached result, STOP
        â”‚ miss
        â–¼
search_structured â”€â”€â–º enough content? â”€â”€â–º use it, STOP
        â”‚ need deeper page
        â–¼
scrape_url (clean_json + strict_relevance + query)
  â”‚ raw .md/.mdx/.rst/.txt URL? â”€â”€â–º HTML pipeline skipped, raw content returned + raw_markdown_url warning
  â”‚
  â”‚ confidence < 0.3 or extraction_score < 0.4?
  â”œâ”€â”€â–º retry with quality_mode: aggressive (CDP rendering)
  â”‚        â”‚ still low? â”€â”€â–º visual_scout â”€â”€â–º human_auth_session (saves session cookies)
  â”‚
  â”‚ need schema-stable JSON?
  â–¼
fetch_then_extract (schema + strict=true)
  â”‚ raw_markdown_url warning? â”€â”€â–º switch to scrape_url(clean_json) instead
  â”‚ confidence == 0.0 + placeholder_page? â”€â”€â–º non_robot_search (no repeat prompt needed)
  â”‚
  â”‚ 403/429/blocked?
  â–¼
proxy_manager grab â”€â”€â–º retry scrape_url with use_proxy: true
        â”‚ still blocked?
        â–¼
non_robot_search  (LAST RESORT â€” persists session after login)
```

---

## Tool Quick-Reference

| Tool (MCP name) | When to use | When NOT to use |
|---|---|---|
| `research_history` | **First step**, before every search/fetch | â€” |
| `search_structured` | Initial research (search + content summaries, single call) | When only raw URLs needed |
| `search_web` | Raw URL list only | As substitute for `search_structured` |
| `scrape_url` | Fetching a specific known URL | As primary research step (use `search_structured` instead) |
| `scrape_url` `clean_json` | Documentation / article pages (token-efficient, noise-free) | Raw `.md`/`.txt` files â€” use `text` or `clean_json` (auto-detected) |
| `scrape_url` `json` | When you need the full structured ScrapeResponse | Large pages without `max_chars` â€” output will be capped at `max_chars` |
| `crawl_website` | Doc site sub-page link discovery | Single-page fetches |
| `fetch_then_extract` | One-shot fetch + strict schema extraction | When you only need prose; use `scrape_url(clean_json)` instead |
| `extract_structured` | Schema extraction; auto-warns on raw markdown URLs | Raw `.md`/`.json`/`.txt` files â€” check for `raw_markdown_url` warning |
| `proxy_manager` | After any 403/429/rate-limit error | Proactively without a block signal |
| `visual_scout` | Screenshot a page to check for auth-walls (`auth_risk_score â‰¥ 0.4`) | General content fetching |
| `human_auth_session` | Full HITL login â€” persists session cookies for future calls | Before trying `non_robot_search` first |
| `non_robot_search` | CAPTCHA / Cloudflare / login wall (last resort) | Any page automatable via proxy rotation |
