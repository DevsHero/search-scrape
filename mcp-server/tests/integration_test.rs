/// Integration Tests: Self-Evolving SDET Suite
/// Tests diverse web patterns to identify extraction failures
use shadowcrawl::rust_scraper::RustScraper;

// Initialize logging for tests
fn init_logger() {
    let _ = tracing_subscriber::fmt()
        .with_max_level(tracing::Level::INFO)
        .with_test_writer()
        .try_init();
}

#[tokio::test]
async fn test_wikipedia_table_extraction() {
    init_logger();
    let scraper = RustScraper::new();
    let url = "https://en.wikipedia.org/wiki/Rust_(programming_language)";

    println!("\nğŸ§ª TEST 1: Wikipedia (Static + Tables)");
    println!("URL: {}", url);

    match scraper.scrape_url(url).await {
        Ok(result) => {
            println!("âœ… Status: {}", result.status_code);
            println!("ğŸ“Š Word Count: {}", result.word_count);
            println!(
                "ğŸ“ˆ Extraction Score: {:.2}",
                result.extraction_score.unwrap_or(0.0)
            );
            println!("ğŸ”¢ Code Blocks: {}", result.code_blocks.len());
            println!("âš ï¸  Warnings: {:?}", result.warnings);

            // Assertions
            assert!(
                result.word_count > 100,
                "âŒ FAIL: Word count too low ({})",
                result.word_count
            );
            assert!(
                result.extraction_score.unwrap_or(0.0) >= 0.6,
                "âŒ FAIL: Extraction score too low ({:.2})",
                result.extraction_score.unwrap_or(0.0)
            );

            // Check for table markers in Markdown
            let has_table_structure =
                result.clean_content.contains("|") || result.clean_content.contains("---");
            println!("ğŸ“‹ Has Table Structure: {}", has_table_structure);

            // Sample first 500 chars
            println!("\nğŸ“„ Content Preview:");
            println!(
                "{}",
                result.clean_content.chars().take(500).collect::<String>()
            );
        }
        Err(e) => {
            panic!("âŒ FAIL: {}", e);
        }
    }
}

#[tokio::test]
async fn test_rust_docs_code_blocks() {
    let scraper = RustScraper::new();
    let url = "https://doc.rust-lang.org/book/ch01-02-hello-world.html";

    println!("\nğŸ§ª TEST 2: Rust Docs (Technical + Code Blocks)");
    println!("URL: {}", url);

    match scraper.scrape_url(url).await {
        Ok(result) => {
            println!("âœ… Status: {}", result.status_code);
            println!("ğŸ“Š Word Count: {}", result.word_count);
            println!(
                "ğŸ“ˆ Extraction Score: {:.2}",
                result.extraction_score.unwrap_or(0.0)
            );
            println!("ğŸ”¢ Code Blocks: {}", result.code_blocks.len());
            println!("âš ï¸  Warnings: {:?}", result.warnings);

            // Assertions
            assert!(
                result.word_count > 50,
                "âŒ FAIL: Word count too low ({})",
                result.word_count
            );
            assert!(
                !result.code_blocks.is_empty(),
                "âŒ FAIL: No code blocks extracted"
            );
            assert!(
                result.extraction_score.unwrap_or(0.0) >= 0.7,
                "âŒ FAIL: Extraction score too low ({:.2})",
                result.extraction_score.unwrap_or(0.0)
            );

            // Check first code block
            if let Some(block) = result.code_blocks.first() {
                println!("\nğŸ’» First Code Block:");
                println!("Language: {:?}", block.language);
                println!("Code: {}", block.code.chars().take(200).collect::<String>());

                assert!(block.code.len() > 10, "âŒ FAIL: Code block too short");
            }

            // Sample content
            println!("\nğŸ“„ Content Preview:");
            println!(
                "{}",
                result.clean_content.chars().take(500).collect::<String>()
            );
        }
        Err(e) => {
            panic!("âŒ FAIL: {}", e);
        }
    }
}

#[tokio::test]
async fn test_github_readme() {
    let scraper = RustScraper::new();
    // Use raw content to avoid GitHub UI noise and intermittent blob-view errors.
    let url = "https://raw.githubusercontent.com/rust-lang/rust/master/README.md";

    println!("\nğŸ§ª TEST 3: GitHub README (Markdown Native)");
    println!("URL: {}", url);

    match scraper.scrape_url(url).await {
        Ok(result) => {
            println!("âœ… Status: {}", result.status_code);
            println!("ğŸ“Š Word Count: {}", result.word_count);
            println!(
                "ğŸ“ˆ Extraction Score: {:.2}",
                result.extraction_score.unwrap_or(0.0)
            );
            println!("ğŸ”¢ Code Blocks: {}", result.code_blocks.len());
            println!("âš ï¸  Warnings: {:?}", result.warnings);

            // Assertions
            // Raw README should be stable and have meaningful content.
            assert!(
                result.status_code < 400,
                "âŒ FAIL: HTTP status was {}",
                result.status_code
            );
            assert!(
                result.word_count > 80,
                "âŒ FAIL: Word count too low ({})",
                result.word_count
            );

            // Check for Markdown structure
            let has_markdown = result.clean_content.contains("##")
                || result.clean_content.contains("**")
                || result.clean_content.contains("```");
            println!("ğŸ“ Has Markdown Structure: {}", has_markdown);

            // Sample content
            println!("\nğŸ“„ Content Preview:");
            println!(
                "{}",
                result.clean_content.chars().take(500).collect::<String>()
            );
        }
        Err(e) => {
            panic!("âŒ FAIL: {}", e);
        }
    }
}

#[tokio::test]
async fn test_reddit_thread_with_native_cdp() {
    init_logger();
    let scraper = RustScraper::new();
    let url = "https://old.reddit.com/r/rust/comments/10nimss/how_do_i_start_learning_rust/";

    println!("\nğŸ§ª TEST 4: Reddit (JS-Heavy + Native CDP Fallback)");
    println!("URL: {}", url);

    let cdp_available = shadowcrawl::scraping::browser_manager::native_browser_available();
    println!("ğŸŒ Native CDP Available: {}", cdp_available);

    match scraper.scrape_url(url).await {
        Ok(result) => {
            println!("âœ… Status: {}", result.status_code);
            println!("ğŸ“Š Word Count: {}", result.word_count);
            println!(
                "ğŸ“ˆ Extraction Score: {:.2}",
                result.extraction_score.unwrap_or(0.0)
            );
            println!("âš ï¸  Warnings: {:?}", result.warnings);

            // For Reddit, we expect lower scores but still some content
            assert!(
                result.word_count > 20,
                "âŒ FAIL: No meaningful content extracted ({})",
                result.word_count
            );

            let _ = cdp_available; // informational only

            // Sample content
            println!("\nğŸ“„ Content Preview:");
            println!(
                "{}",
                result.clean_content.chars().take(500).collect::<String>()
            );
        }
        Err(e) => {
            println!("âš ï¸  Expected potential failure for Reddit: {}", e);
        }
    }
}

#[tokio::test]
async fn test_medium_article() {
    let scraper = RustScraper::new();
    let url = "https://medium.com/@benwubbleyou/learn-rust-the-dangerous-way-44e9efd7cbe";

    println!("\nğŸ§ª TEST 5: Medium (Article + Paywall)");
    println!("URL: {}", url);

    match scraper.scrape_url(url).await {
        Ok(result) => {
            println!("âœ… Status: {}", result.status_code);
            println!("ğŸ“Š Word Count: {}", result.word_count);
            println!(
                "ğŸ“ˆ Extraction Score: {:.2}",
                result.extraction_score.unwrap_or(0.0)
            );
            println!("ğŸ‘¤ Author: {:?}", result.author);
            println!("ğŸ“… Published: {:?}", result.published_at);
            println!("âš ï¸  Warnings: {:?}", result.warnings);

            // Assertions - Medium paywall limitations acknowledged
            // Medium uses React SSR + paywall; 20-40 words is realistic without JS rendering
            assert!(
                result.word_count > 20,
                "âŒ FAIL: Word count too low ({})",
                result.word_count
            );

            // Sample content
            println!("\nğŸ“„ Content Preview:");
            println!(
                "{}",
                result.clean_content.chars().take(500).collect::<String>()
            );
        }
        Err(e) => {
            println!("âš ï¸  Medium may block: {}", e);
        }
    }
}

#[tokio::test]
async fn test_docs_portal() {
    let scraper = RustScraper::new();
    let url = "https://developer.mozilla.org/en-US/docs/Web";

    println!("\nğŸ§ª TEST 6: Docs Portal (Enterprise Docs)");
    println!("URL: {}", url);

    match scraper.scrape_url(url).await {
        Ok(result) => {
            println!("âœ… Status: {}", result.status_code);
            println!("ğŸ“Š Word Count: {}", result.word_count);
            println!(
                "ğŸ“ˆ Extraction Score: {:.2}",
                result.extraction_score.unwrap_or(0.0)
            );
            println!("ğŸ”¢ Code Blocks: {}", result.code_blocks.len());
            println!("ğŸ”— Links: {}", result.links.len());
            println!("âš ï¸  Warnings: {:?}", result.warnings);

            // Assertions
            assert!(
                result.word_count > 100,
                "âŒ FAIL: Word count too low ({})",
                result.word_count
            );
            assert!(
                result.extraction_score.unwrap_or(0.0) >= 0.45,
                "âŒ FAIL: Extraction score too low ({:.2})",
                result.extraction_score.unwrap_or(0.0)
            );

            // Sample content
            println!("\nğŸ“„ Content Preview:");
            println!(
                "{}",
                result.clean_content.chars().take(500).collect::<String>()
            );
        }
        Err(e) => {
            panic!("âŒ FAIL: {}", e);
        }
    }
}

#[tokio::test]
#[ignore] // Only run when a local browser is available
async fn test_native_cdp_direct() {
    init_logger();
    let scraper = RustScraper::new();

    // Test with a JS-heavy SPA that requires rendering
    let url = "https://www.npmjs.com/package/react";

    println!("\nğŸ§ª TEST 7: Native CDP Direct (JS-Heavy SPA)");
    println!("URL: {}", url);

    // Check if native CDP is available
    if !shadowcrawl::scraping::browser_manager::native_browser_available() {
        println!("âš ï¸  Skipping: no local browser found");
        println!("   Install Brave/Chrome/Chromium or set CHROME_EXECUTABLE to enable");
        return;
    }

    match scraper.scrape_with_browserless(url).await {
        Ok(result) => {
            println!("âœ… Status: {}", result.status_code);
            println!("ğŸ“Š Word Count: {}", result.word_count);
            println!(
                "ğŸ“ˆ Extraction Score: {:.2}",
                result.extraction_score.unwrap_or(0.0)
            );
            println!("âš ï¸  Warnings: {:?}", result.warnings);

            // Should extract meaningful content from JS-rendered page
            assert!(
                result.word_count > 50,
                "âŒ FAIL: Insufficient content extracted ({})",
                result.word_count
            );

            println!("\nâœ… Native CDP successfully rendered JS-heavy content");

            // Sample content
            println!("\nğŸ“„ Content Preview:");
            println!(
                "{}",
                result.clean_content.chars().take(500).collect::<String>()
            );
        }
        Err(e) => {
            panic!("âŒ FAIL: Native CDP scraping failed: {}", e);
        }
    }
}

// NOTE: Keep this file focused on executable validations.
